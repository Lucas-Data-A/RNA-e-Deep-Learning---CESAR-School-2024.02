{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Aluno: Lucas Barbosa dos Santos"
      ],
      "metadata": {
        "id": "H1SM3Ye9rQvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Atividade\n",
        "Utilizando como base o código 2.2-LinearRegressionIrisNumPy.ipynb,\n",
        "faça as 3 atividades abaixo. Para submissão, submeta sua resposta no\n",
        "repositório criado para a disciplina.\n",
        "\n",
        "Coloque o resultado dentro de uma pasta chamada \"Atividade Aula 2.2\".\n",
        "\n",
        "1. Modifique o laço do treinamento via gradiente descendente para guardar em uma\n",
        "lista, o valor da perda no final de cada época. Após o treinamento,\n",
        "plote o valor da perda em função da época.\n",
        "\n",
        "2. Calcule o valor da perda (MSE) da rede com os parâmetros inicializados, sem serem treinados.\n",
        "\n",
        "3. Coloque os valores dos pesos da solução ótima analítica no modelo da rede e\n",
        "calcule o valor da perda (MSE) e compare com o valor da perda obtida\n",
        "pelo método da otimização via gradiente descendente."
      ],
      "metadata": {
        "id": "5OU9KCOSrWxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_UvYQ71oq-6N"
      },
      "outputs": [],
      "source": [
        "#Importação de Pacotes\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt   # para plotting\n",
        "from sklearn.datasets import load_iris  # para carregar dataset\n",
        "import numpy as np  # processamento matricial\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leitura dos dados\n",
        "iris = load_iris()\n",
        "data = iris.data[iris.target==1,::2]  # comprimento das sépalas e pétalas, indices 0 e 2\n",
        "x_train = data[:,0:1]\n",
        "y_train = data[:,1:2]\n",
        "n_samples = x_train.shape[0]\n",
        "print('x_train.shape:',x_train.shape, x_train.dtype)\n",
        "print('y_train.shape:',y_train.shape, y_train.dtype)\n",
        "print('x_train[:5]:\\n', x_train[:5])\n",
        "print('y_train[:5]:\\n', y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stipPz3iriPd",
        "outputId": "aa8b9d63-5e78-40d1-875b-fbc7857822b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape: (50, 1) float64\n",
            "y_train.shape: (50, 1) float64\n",
            "x_train[:5]:\n",
            " [[7. ]\n",
            " [6.4]\n",
            " [6.9]\n",
            " [5.5]\n",
            " [6.5]]\n",
            "y_train[:5]:\n",
            " [[4.7]\n",
            " [4.5]\n",
            " [4.9]\n",
            " [4. ]\n",
            " [4.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalização dos dados\n",
        "x_train -= x_train.min()\n",
        "x_train /= x_train.max()\n",
        "y_train -= y_train.min()\n",
        "y_train /= y_train.max()"
      ],
      "metadata": {
        "id": "ipnTsajVrynt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenchendo x com coluna de 1's para bias\n",
        "x_train_bias = np.hstack([np.ones(shape=(n_samples,1)), x_train])\n",
        "x_train_bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES5dNePJsIeJ",
        "outputId": "aa7f44e4-e32d-4511-eb3f-92aa4b360e9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        ],\n",
              "       [1.        , 0.71428571],\n",
              "       [1.        , 0.95238095],\n",
              "       [1.        , 0.28571429],\n",
              "       [1.        , 0.76190476],\n",
              "       [1.        , 0.38095238],\n",
              "       [1.        , 0.66666667],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.80952381],\n",
              "       [1.        , 0.14285714],\n",
              "       [1.        , 0.04761905],\n",
              "       [1.        , 0.47619048],\n",
              "       [1.        , 0.52380952],\n",
              "       [1.        , 0.57142857],\n",
              "       [1.        , 0.33333333],\n",
              "       [1.        , 0.85714286],\n",
              "       [1.        , 0.33333333],\n",
              "       [1.        , 0.42857143],\n",
              "       [1.        , 0.61904762],\n",
              "       [1.        , 0.33333333],\n",
              "       [1.        , 0.47619048],\n",
              "       [1.        , 0.57142857],\n",
              "       [1.        , 0.66666667],\n",
              "       [1.        , 0.57142857],\n",
              "       [1.        , 0.71428571],\n",
              "       [1.        , 0.80952381],\n",
              "       [1.        , 0.9047619 ],\n",
              "       [1.        , 0.85714286],\n",
              "       [1.        , 0.52380952],\n",
              "       [1.        , 0.38095238],\n",
              "       [1.        , 0.28571429],\n",
              "       [1.        , 0.28571429],\n",
              "       [1.        , 0.42857143],\n",
              "       [1.        , 0.52380952],\n",
              "       [1.        , 0.23809524],\n",
              "       [1.        , 0.52380952],\n",
              "       [1.        , 0.85714286],\n",
              "       [1.        , 0.66666667],\n",
              "       [1.        , 0.33333333],\n",
              "       [1.        , 0.28571429],\n",
              "       [1.        , 0.28571429],\n",
              "       [1.        , 0.57142857],\n",
              "       [1.        , 0.42857143],\n",
              "       [1.        , 0.04761905],\n",
              "       [1.        , 0.33333333],\n",
              "       [1.        , 0.38095238],\n",
              "       [1.        , 0.38095238],\n",
              "       [1.        , 0.61904762],\n",
              "       [1.        , 0.0952381 ],\n",
              "       [1.        , 0.38095238]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classe com o modelo da rede\n",
        "class Net():\n",
        "    def __init__(self, n_in, n_out):\n",
        "        self.w = np.random.uniform(-0.1,0.1,(n_out,n_in)) #inicialização dos parâmetros\n",
        "\n",
        "    def forward(self, x_bias):\n",
        "        return x_bias.dot(self.w.T) #^y"
      ],
      "metadata": {
        "id": "sZcySxu8sSnY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a rede, instanciando o objeto model\n",
        "model = Net(2,1) # duas entradas (1 + x0) e uma saída y_pred"
      ],
      "metadata": {
        "id": "OZ98BEaxsSkY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 0.5\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # forward - predict\n",
        "    y_pred = model.forward(x_train_bias)\n",
        "\n",
        "    #loss cálculo da função de perda\n",
        "    loss = np.square(y_pred - y_train).mean()\n",
        "\n",
        "    # cálculo do gradiente pelas derivadas parciais\n",
        "    w_grad = (2.0/n_samples) * (x_train_bias.T).dot(x_train_bias.dot(model.w.T) - y_train)\n",
        "\n",
        "    # gradiente descendente\n",
        "    model.w = model.w - learning_rate * w_grad.T\n",
        "\n",
        "    # verbose\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print('Epoch[{}/{}], loss: {:.6f}'\n",
        "              .format(epoch+1, num_epochs, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MftElpdlsSeY",
        "outputId": "5de3f49f-5c96-4cf7-91cb-52cb929c56d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/100], loss: 0.469163\n",
            "Epoch[2/100], loss: 0.058070\n",
            "Epoch[3/100], loss: 0.030543\n",
            "Epoch[4/100], loss: 0.028105\n",
            "Epoch[5/100], loss: 0.027364\n",
            "Epoch[6/100], loss: 0.026787\n",
            "Epoch[7/100], loss: 0.026269\n",
            "Epoch[8/100], loss: 0.025799\n",
            "Epoch[9/100], loss: 0.025373\n",
            "Epoch[10/100], loss: 0.024986\n",
            "Epoch[11/100], loss: 0.024634\n",
            "Epoch[12/100], loss: 0.024315\n",
            "Epoch[13/100], loss: 0.024025\n",
            "Epoch[14/100], loss: 0.023762\n",
            "Epoch[15/100], loss: 0.023524\n",
            "Epoch[16/100], loss: 0.023307\n",
            "Epoch[17/100], loss: 0.023110\n",
            "Epoch[18/100], loss: 0.022931\n",
            "Epoch[19/100], loss: 0.022769\n",
            "Epoch[20/100], loss: 0.022622\n",
            "Epoch[21/100], loss: 0.022488\n",
            "Epoch[22/100], loss: 0.022366\n",
            "Epoch[23/100], loss: 0.022256\n",
            "Epoch[24/100], loss: 0.022156\n",
            "Epoch[25/100], loss: 0.022065\n",
            "Epoch[26/100], loss: 0.021983\n",
            "Epoch[27/100], loss: 0.021908\n",
            "Epoch[28/100], loss: 0.021840\n",
            "Epoch[29/100], loss: 0.021778\n",
            "Epoch[30/100], loss: 0.021722\n",
            "Epoch[31/100], loss: 0.021671\n",
            "Epoch[32/100], loss: 0.021625\n",
            "Epoch[33/100], loss: 0.021583\n",
            "Epoch[34/100], loss: 0.021545\n",
            "Epoch[35/100], loss: 0.021510\n",
            "Epoch[36/100], loss: 0.021479\n",
            "Epoch[37/100], loss: 0.021450\n",
            "Epoch[38/100], loss: 0.021425\n",
            "Epoch[39/100], loss: 0.021401\n",
            "Epoch[40/100], loss: 0.021380\n",
            "Epoch[41/100], loss: 0.021360\n",
            "Epoch[42/100], loss: 0.021343\n",
            "Epoch[43/100], loss: 0.021327\n",
            "Epoch[44/100], loss: 0.021312\n",
            "Epoch[45/100], loss: 0.021299\n",
            "Epoch[46/100], loss: 0.021287\n",
            "Epoch[47/100], loss: 0.021276\n",
            "Epoch[48/100], loss: 0.021267\n",
            "Epoch[49/100], loss: 0.021258\n",
            "Epoch[50/100], loss: 0.021249\n",
            "Epoch[51/100], loss: 0.021242\n",
            "Epoch[52/100], loss: 0.021235\n",
            "Epoch[53/100], loss: 0.021229\n",
            "Epoch[54/100], loss: 0.021224\n",
            "Epoch[55/100], loss: 0.021219\n",
            "Epoch[56/100], loss: 0.021214\n",
            "Epoch[57/100], loss: 0.021210\n",
            "Epoch[58/100], loss: 0.021206\n",
            "Epoch[59/100], loss: 0.021203\n",
            "Epoch[60/100], loss: 0.021200\n",
            "Epoch[61/100], loss: 0.021197\n",
            "Epoch[62/100], loss: 0.021195\n",
            "Epoch[63/100], loss: 0.021192\n",
            "Epoch[64/100], loss: 0.021190\n",
            "Epoch[65/100], loss: 0.021188\n",
            "Epoch[66/100], loss: 0.021186\n",
            "Epoch[67/100], loss: 0.021185\n",
            "Epoch[68/100], loss: 0.021183\n",
            "Epoch[69/100], loss: 0.021182\n",
            "Epoch[70/100], loss: 0.021181\n",
            "Epoch[71/100], loss: 0.021180\n",
            "Epoch[72/100], loss: 0.021179\n",
            "Epoch[73/100], loss: 0.021178\n",
            "Epoch[74/100], loss: 0.021177\n",
            "Epoch[75/100], loss: 0.021177\n",
            "Epoch[76/100], loss: 0.021176\n",
            "Epoch[77/100], loss: 0.021175\n",
            "Epoch[78/100], loss: 0.021175\n",
            "Epoch[79/100], loss: 0.021174\n",
            "Epoch[80/100], loss: 0.021174\n",
            "Epoch[81/100], loss: 0.021173\n",
            "Epoch[82/100], loss: 0.021173\n",
            "Epoch[83/100], loss: 0.021173\n",
            "Epoch[84/100], loss: 0.021172\n",
            "Epoch[85/100], loss: 0.021172\n",
            "Epoch[86/100], loss: 0.021172\n",
            "Epoch[87/100], loss: 0.021172\n",
            "Epoch[88/100], loss: 0.021171\n",
            "Epoch[89/100], loss: 0.021171\n",
            "Epoch[90/100], loss: 0.021171\n",
            "Epoch[91/100], loss: 0.021171\n",
            "Epoch[92/100], loss: 0.021171\n",
            "Epoch[93/100], loss: 0.021171\n",
            "Epoch[94/100], loss: 0.021171\n",
            "Epoch[95/100], loss: 0.021170\n",
            "Epoch[96/100], loss: 0.021170\n",
            "Epoch[97/100], loss: 0.021170\n",
            "Epoch[98/100], loss: 0.021170\n",
            "Epoch[99/100], loss: 0.021170\n",
            "Epoch[100/100], loss: 0.021170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros treinados\n",
        "print(model.w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKkONffWsSbQ",
        "outputId": "b03a87ee-7cfa-4350-ebf8-c699d9e07698"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.26295353 0.68335641]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo\n",
        "loss = np.square(y_pred - y_train).mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzaZGX6UsSYQ",
        "outputId": "92aa806f-eab1-4be5-ff91-b593c1a089e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.021170062593862105)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Valor ótimo, solução analítica\n",
        "x = x_train_bias\n",
        "y = y_train\n",
        "w_opt = (np.linalg.inv((x.T).dot(x)).dot(x.T)).dot(y)\n",
        "print(w_opt.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTC_cDnksSVR",
        "outputId": "6c3ba3ef-c2be-4efd-e03c-3b7cfd855fbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.26134159 0.68646976]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.5\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # forward - predict\n",
        "    y_pred = model.forward(x_train_bias)\n",
        "\n",
        "    #loss cálculo da função de perda\n",
        "    loss = np.square(y_pred - y_train).mean()\n",
        "\n",
        "    # cálculo do gradiente pelas derivadas parciais\n",
        "    w_grad = (2.0/n_samples) * (x_train_bias.T).dot(x_train_bias.dot(model.w.T) - y_train)\n",
        "\n",
        "    # gradiente descendente\n",
        "    model.w = model.w - learning_rate * w_grad.T\n",
        "\n",
        "    losses.append(loss)\n",
        "\n",
        "    # verbose\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print('Epoch[{}/{}], loss: {:.6f}'\n",
        "              .format(epoch+1, num_epochs, loss))\n",
        "print(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5JiWqrQtW6m",
        "outputId": "18ff58f4-811d-4901-faa9-2f432dc83ad3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/100], loss: 0.021170\n",
            "Epoch[2/100], loss: 0.021170\n",
            "Epoch[3/100], loss: 0.021170\n",
            "Epoch[4/100], loss: 0.021170\n",
            "Epoch[5/100], loss: 0.021170\n",
            "Epoch[6/100], loss: 0.021170\n",
            "Epoch[7/100], loss: 0.021170\n",
            "Epoch[8/100], loss: 0.021170\n",
            "Epoch[9/100], loss: 0.021170\n",
            "Epoch[10/100], loss: 0.021170\n",
            "Epoch[11/100], loss: 0.021170\n",
            "Epoch[12/100], loss: 0.021170\n",
            "Epoch[13/100], loss: 0.021170\n",
            "Epoch[14/100], loss: 0.021170\n",
            "Epoch[15/100], loss: 0.021170\n",
            "Epoch[16/100], loss: 0.021170\n",
            "Epoch[17/100], loss: 0.021170\n",
            "Epoch[18/100], loss: 0.021170\n",
            "Epoch[19/100], loss: 0.021170\n",
            "Epoch[20/100], loss: 0.021170\n",
            "Epoch[21/100], loss: 0.021170\n",
            "Epoch[22/100], loss: 0.021170\n",
            "Epoch[23/100], loss: 0.021169\n",
            "Epoch[24/100], loss: 0.021169\n",
            "Epoch[25/100], loss: 0.021169\n",
            "Epoch[26/100], loss: 0.021169\n",
            "Epoch[27/100], loss: 0.021169\n",
            "Epoch[28/100], loss: 0.021169\n",
            "Epoch[29/100], loss: 0.021169\n",
            "Epoch[30/100], loss: 0.021169\n",
            "Epoch[31/100], loss: 0.021169\n",
            "Epoch[32/100], loss: 0.021169\n",
            "Epoch[33/100], loss: 0.021169\n",
            "Epoch[34/100], loss: 0.021169\n",
            "Epoch[35/100], loss: 0.021169\n",
            "Epoch[36/100], loss: 0.021169\n",
            "Epoch[37/100], loss: 0.021169\n",
            "Epoch[38/100], loss: 0.021169\n",
            "Epoch[39/100], loss: 0.021169\n",
            "Epoch[40/100], loss: 0.021169\n",
            "Epoch[41/100], loss: 0.021169\n",
            "Epoch[42/100], loss: 0.021169\n",
            "Epoch[43/100], loss: 0.021169\n",
            "Epoch[44/100], loss: 0.021169\n",
            "Epoch[45/100], loss: 0.021169\n",
            "Epoch[46/100], loss: 0.021169\n",
            "Epoch[47/100], loss: 0.021169\n",
            "Epoch[48/100], loss: 0.021169\n",
            "Epoch[49/100], loss: 0.021169\n",
            "Epoch[50/100], loss: 0.021169\n",
            "Epoch[51/100], loss: 0.021169\n",
            "Epoch[52/100], loss: 0.021169\n",
            "Epoch[53/100], loss: 0.021169\n",
            "Epoch[54/100], loss: 0.021169\n",
            "Epoch[55/100], loss: 0.021169\n",
            "Epoch[56/100], loss: 0.021169\n",
            "Epoch[57/100], loss: 0.021169\n",
            "Epoch[58/100], loss: 0.021169\n",
            "Epoch[59/100], loss: 0.021169\n",
            "Epoch[60/100], loss: 0.021169\n",
            "Epoch[61/100], loss: 0.021169\n",
            "Epoch[62/100], loss: 0.021169\n",
            "Epoch[63/100], loss: 0.021169\n",
            "Epoch[64/100], loss: 0.021169\n",
            "Epoch[65/100], loss: 0.021169\n",
            "Epoch[66/100], loss: 0.021169\n",
            "Epoch[67/100], loss: 0.021169\n",
            "Epoch[68/100], loss: 0.021169\n",
            "Epoch[69/100], loss: 0.021169\n",
            "Epoch[70/100], loss: 0.021169\n",
            "Epoch[71/100], loss: 0.021169\n",
            "Epoch[72/100], loss: 0.021169\n",
            "Epoch[73/100], loss: 0.021169\n",
            "Epoch[74/100], loss: 0.021169\n",
            "Epoch[75/100], loss: 0.021169\n",
            "Epoch[76/100], loss: 0.021169\n",
            "Epoch[77/100], loss: 0.021169\n",
            "Epoch[78/100], loss: 0.021169\n",
            "Epoch[79/100], loss: 0.021169\n",
            "Epoch[80/100], loss: 0.021169\n",
            "Epoch[81/100], loss: 0.021169\n",
            "Epoch[82/100], loss: 0.021169\n",
            "Epoch[83/100], loss: 0.021169\n",
            "Epoch[84/100], loss: 0.021169\n",
            "Epoch[85/100], loss: 0.021169\n",
            "Epoch[86/100], loss: 0.021169\n",
            "Epoch[87/100], loss: 0.021169\n",
            "Epoch[88/100], loss: 0.021169\n",
            "Epoch[89/100], loss: 0.021169\n",
            "Epoch[90/100], loss: 0.021169\n",
            "Epoch[91/100], loss: 0.021169\n",
            "Epoch[92/100], loss: 0.021169\n",
            "Epoch[93/100], loss: 0.021169\n",
            "Epoch[94/100], loss: 0.021169\n",
            "Epoch[95/100], loss: 0.021169\n",
            "Epoch[96/100], loss: 0.021169\n",
            "Epoch[97/100], loss: 0.021169\n",
            "Epoch[98/100], loss: 0.021169\n",
            "Epoch[99/100], loss: 0.021169\n",
            "Epoch[100/100], loss: 0.021169\n",
            "[np.float64(0.02117000379151052), np.float64(0.02116995040474736), np.float64(0.02116990193480691), np.float64(0.021169857928858827), np.float64(0.021169817975777626), np.float64(0.021169781702301722), np.float64(0.021169748769546245), np.float64(0.021169718869836967), np.float64(0.021169691723835923), np.float64(0.021169667077931654), np.float64(0.021169644701869838), np.float64(0.021169624386602165), np.float64(0.021169605942333282), np.float64(0.02116958919674764), np.float64(0.02116957399339963), np.float64(0.02116956019025202), np.float64(0.021169547658348917), np.float64(0.02116953628061104), np.float64(0.021169525950741893), np.float64(0.021169516572234696), np.float64(0.02116950805747077), np.float64(0.021169500326900938), np.float64(0.02116949330830237), np.float64(0.021169486936103817), np.float64(0.021169481150773037), np.float64(0.021169475898260574), np.float64(0.021169471129494858), np.float64(0.02116946679992369), np.float64(0.021169462869098064), np.float64(0.02116945930029425), np.float64(0.0211694560601707), np.float64(0.021169453118456572), np.float64(0.02116945044766891), np.float64(0.021169448022855867), np.float64(0.02116944582136366), np.float64(0.021169443822624825), np.float64(0.02116944200796617), np.float64(0.021169440360434232), np.float64(0.021169438864636958), np.float64(0.021169437506599862), np.float64(0.021169436273635482), np.float64(0.02116943515422486), np.float64(0.021169434137909896), np.float64(0.021169433215195665), np.float64(0.02116943237746172), np.float64(0.02116943161688153), np.float64(0.02116943092634938), np.float64(0.02116943029941398), np.float64(0.021169429730218164), np.float64(0.021169429213444238), np.float64(0.02116942874426424), np.float64(0.021169428318294867), np.float64(0.021169427931556475), np.float64(0.021169427580435985), np.float64(0.021169427261653048), np.float64(0.021169426972229427), np.float64(0.021169426709461193), np.float64(0.021169426470893433), np.float64(0.021169426254297317), np.float64(0.02116942605764931), np.float64(0.021169425879112226), np.float64(0.021169425717018072), np.float64(0.021169425569852494), np.float64(0.021169425436240594), np.float64(0.02116942531493411), np.float64(0.021169425204799724), np.float64(0.02116942510480851), np.float64(0.021169425014026313), np.float64(0.02116942493160498), np.float64(0.02116942485677451), np.float64(0.021169424788835777), np.float64(0.02116942472715409), np.float64(0.02116942467115316), np.float64(0.021169424620309817), np.float64(0.021169424574149054), np.float64(0.02116942453223961), np.float64(0.021169424494189952), np.float64(0.0211694244596446), np.float64(0.021169424428280802), np.float64(0.02116942439980556), np.float64(0.021169424373952826), np.float64(0.021169424350481095), np.float64(0.02116942432917106), np.float64(0.021169424309823638), np.float64(0.02116942429225808), np.float64(0.02116942427631028), np.float64(0.021169424261831243), np.float64(0.02116942424868569), np.float64(0.02116942423675083), np.float64(0.021169424225915147), np.float64(0.0211694242160774), np.float64(0.021169424207145696), np.float64(0.021169424199036592), np.float64(0.021169424191674314), np.float64(0.021169424184990092), np.float64(0.021169424178921474), np.float64(0.02116942417341177), np.float64(0.021169424168409487), np.float64(0.021169424163867915), np.float64(0.021169424159744613)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "model = Net(2,1)\n",
        "print(model.w)\n",
        "y_pred = model.forward(x_train_bias)\n",
        "print(y_pred)\n",
        "loss = np.square(y_pred - y_train).mean()\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0T2088KtW4o",
        "outputId": "8bdd382f-e778-40e6-dab9-f61d9c51ac08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03669259 0.04254041]]\n",
            "[[0.07923299]\n",
            " [0.06707859]\n",
            " [0.07720726]\n",
            " [0.04884699]\n",
            " [0.06910432]\n",
            " [0.05289846]\n",
            " [0.06505286]\n",
            " [0.03669259]\n",
            " [0.07113006]\n",
            " [0.04276979]\n",
            " [0.03871832]\n",
            " [0.05694992]\n",
            " [0.05897566]\n",
            " [0.06100139]\n",
            " [0.05087272]\n",
            " [0.07315579]\n",
            " [0.05087272]\n",
            " [0.05492419]\n",
            " [0.06302712]\n",
            " [0.05087272]\n",
            " [0.05694992]\n",
            " [0.06100139]\n",
            " [0.06505286]\n",
            " [0.06100139]\n",
            " [0.06707859]\n",
            " [0.07113006]\n",
            " [0.07518153]\n",
            " [0.07315579]\n",
            " [0.05897566]\n",
            " [0.05289846]\n",
            " [0.04884699]\n",
            " [0.04884699]\n",
            " [0.05492419]\n",
            " [0.05897566]\n",
            " [0.04682125]\n",
            " [0.05897566]\n",
            " [0.07315579]\n",
            " [0.06505286]\n",
            " [0.05087272]\n",
            " [0.04884699]\n",
            " [0.04884699]\n",
            " [0.06100139]\n",
            " [0.05492419]\n",
            " [0.03871832]\n",
            " [0.05087272]\n",
            " [0.05289846]\n",
            " [0.05289846]\n",
            " [0.06302712]\n",
            " [0.04074405]\n",
            " [0.05289846]]\n",
            "0.3398312818108879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "#Solução gradiente\n",
        "model = Net(2,1)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.5\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # forward - predict\n",
        "    y_pred = model.forward(x_train_bias)\n",
        "\n",
        "    #loss cálculo da função de perda\n",
        "    loss = np.square(y_pred - y_train).mean()\n",
        "\n",
        "    # cálculo do gradiente pelas derivadas parciais\n",
        "    w_grad = (2.0/n_samples) * (x_train_bias.T).dot(x_train_bias.dot(model.w.T) - y_train)\n",
        "\n",
        "    # gradiente descendente\n",
        "    model.w = model.w - learning_rate * w_grad.T\n",
        "print(f\"Valores Solução Gradiente:\\n w: {model.w} | perda: {loss}\")\n",
        "\n",
        "#Solução ótima\n",
        "x = x_train_bias\n",
        "y = y_train\n",
        "w_opt = (np.linalg.inv((x.T).dot(x)).dot(x.T)).dot(y)\n",
        "model.w = w_opt.T\n",
        "# forward - predict\n",
        "y_pred = model.forward(x_train_bias)\n",
        "\n",
        "#loss cálculo da função de perda\n",
        "loss = np.square(y_pred - y_train).mean()\n",
        "\n",
        "print(f\"Valores Solução Ótima:\\n w: {model.w} | perda: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUgfnV1xtW1f",
        "outputId": "6c11d584-578c-44f9-bd61-9e1dc8577a6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores Solução Gradiente:\n",
            " w: [[0.26305351 0.6831633 ]] | perda: 0.02117014425438359\n",
            "Valores Solução Ótima:\n",
            " w: [[0.26134159 0.68646976]] | perda: 0.021169424119097165\n"
          ]
        }
      ]
    }
  ]
}